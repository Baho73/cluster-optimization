{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_In70bdlRO1"
   },
   "source": [
    "# Cluster Optimization for Embeddings Data\n",
    "\n",
    "This notebook implements:\n",
    "1. Data cleaning to remove noise items\n",
    "2. Methods to find the optimal number of clusters\n",
    "3. Final clustering with the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqoowiiDlRO5"
   },
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MhxwoGDlRO6"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFNLkeuDlRO8",
    "outputId": "a5600ed2-ef94-449b-fe40-ffc4880a459c"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('data_w_embeddings.csv')\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kCri-YHlRO9",
    "outputId": "b40ccff8-0c7c-4486-fd86-d2b32f169728"
   },
   "outputs": [],
   "source": [
    "# Filter out rows with null embeddings\n",
    "df_filtered = df[df['embedding'].notnull()]\n",
    "print(f\"Filtered data shape: {df_filtered.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_filtered.shape[0]} rows with null embeddings\")\n",
    "\n",
    "# Convert embeddings from strings to numpy arrays\n",
    "df_filtered[\"embedding\"] = df_filtered.embedding.apply(literal_eval).apply(np.array)\n",
    "\n",
    "# Create embedding matrix\n",
    "matrix = np.vstack(df_filtered.embedding.values)\n",
    "print(f\"Embedding matrix shape: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0vGSLP3lRO9"
   },
   "source": [
    "## 2. Data Cleaning - Removing Noise Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3s8lHprlRO-"
   },
   "source": [
    "### 2.1 Method 1: Distance-based Outlier Detection\n",
    "\n",
    "We'll calculate the average distance of each point to its k nearest neighbors and remove points with unusually large distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-meO9selRO-",
    "outputId": "83b03ade-9653-4c97-f414-6dd0c4b0636e"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def detect_distance_outliers(matrix, n_neighbors=50, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Detect outliers based on the average distance to k nearest neighbors.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix: Embedding matrix\n",
    "    - n_neighbors: Number of neighbors to consider\n",
    "    - threshold: Threshold for z-score to identify outliers\n",
    "\n",
    "    Returns:\n",
    "    - is_inlier: Boolean array indicating which points are inliers\n",
    "    \"\"\"\n",
    "    # Find k nearest neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(matrix)\n",
    "    distances, _ = nbrs.kneighbors(matrix)\n",
    "\n",
    "    # Calculate average distance to k nearest neighbors\n",
    "    avg_distances = distances[:, 1:].mean(axis=1)  # Exclude the point itself (distance=0)\n",
    "\n",
    "    # Calculate z-scores\n",
    "    z_scores = (avg_distances - avg_distances.mean()) / avg_distances.std()\n",
    "\n",
    "    # Identify outliers\n",
    "    is_inlier = z_scores < threshold\n",
    "\n",
    "    return is_inlier\n",
    "\n",
    "# Apply distance-based outlier detection\n",
    "is_inlier_distance = detect_distance_outliers(matrix)\n",
    "print(f\"Identified {(~is_inlier_distance).sum()} outliers out of {matrix.shape[0]} points using distance-based method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjQH98MGlRO-"
   },
   "source": [
    "### 2.2 Method 2: Local Outlier Factor (LOF)\n",
    "\n",
    "LOF measures the local deviation of density of a given sample with respect to its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXR1n0QllRO_",
    "outputId": "bbe32c0c-2fa8-447a-9ca7-b262768cb331"
   },
   "outputs": [],
   "source": [
    "# Apply Local Outlier Factor\n",
    "lof = LocalOutlierFactor(n_neighbors=50, contamination=0.1)\n",
    "is_inlier_lof = lof.fit_predict(matrix) == 1  # 1 for inliers, -1 for outliers\n",
    "print(f\"Identified {(~is_inlier_lof).sum()} outliers out of {matrix.shape[0]} points using LOF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0wcf1lclRO_"
   },
   "source": [
    "### 2.3 Method 3: Isolation Forest\n",
    "\n",
    "Isolation Forest isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAo6oWOWlRO_",
    "outputId": "fdd342fe-a9e3-471c-dee5-128ebc5e7d52"
   },
   "outputs": [],
   "source": [
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "is_inlier_isoforest = iso_forest.fit_predict(matrix) == 1  # 1 for inliers, -1 for outliers\n",
    "print(f\"Identified {(~is_inlier_isoforest).sum()} outliers out of {matrix.shape[0]} points using Isolation Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_xM94G6lRPA"
   },
   "source": [
    "### 2.4 Combine Methods and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hg_zKPn0lRPA",
    "outputId": "b5173a51-27b8-4aa3-9e4b-3ac3686bbe1c"
   },
   "outputs": [],
   "source": [
    "# Combine results from all methods (conservative approach - a point is an outlier if identified by at least 2 methods)\n",
    "outlier_votes = (~is_inlier_distance).astype(int) + (~is_inlier_lof).astype(int) + (~is_inlier_isoforest).astype(int)\n",
    "is_inlier_combined = outlier_votes < 2\n",
    "\n",
    "print(f\"Identified {(~is_inlier_combined).sum()} outliers out of {matrix.shape[0]} points using combined methods\")\n",
    "\n",
    "# Create a clean dataset\n",
    "df_clean = df_filtered[is_inlier_combined].copy()\n",
    "print(f\"Clean data shape: {df_clean.shape}\")\n",
    "\n",
    "# Create clean embedding matrix\n",
    "matrix_clean = np.vstack(df_clean.embedding.values)\n",
    "print(f\"Clean embedding matrix shape: {matrix_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwXL_YjglRPA",
    "outputId": "a4a301ed-98f4-43d8-c382-c69eae0bdc7f"
   },
   "outputs": [],
   "source": [
    "# Visualize original vs. clean data using t-SNE\n",
    "def plot_tsne_comparison(matrix_original, matrix_clean, perplexity=30):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Original data\n",
    "    tsne_original = TSNE(n_components=2, perplexity=perplexity, random_state=42, init=\"random\", learning_rate=200)\n",
    "    vis_original = tsne_original.fit_transform(matrix_original)\n",
    "    axes[0].scatter(vis_original[:, 0], vis_original[:, 1], alpha=0.5)\n",
    "    axes[0].set_title(f\"Original Data (n={matrix_original.shape[0]})\")\n",
    "\n",
    "    # Clean data\n",
    "    tsne_clean = TSNE(n_components=2, perplexity=perplexity, random_state=42, init=\"random\", learning_rate=200)\n",
    "    vis_clean = tsne_clean.fit_transform(matrix_clean)\n",
    "    axes[1].scatter(vis_clean[:, 0], vis_clean[:, 1], alpha=0.5)\n",
    "    axes[1].set_title(f\"Clean Data (n={matrix_clean.shape[0]})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot comparison\n",
    "plot_tsne_comparison(matrix, matrix_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-F8Yni-lRPB"
   },
   "source": [
    "## 3. Finding Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTKLuDoKlRPB"
   },
   "source": [
    "### 3.1 Elbow Method\n",
    "\n",
    "The elbow method looks at the percentage of variance explained as a function of the number of clusters. The optimal number of clusters is the one where adding another cluster doesn't give much better modeling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byD9uppflRPC",
    "outputId": "a522e4ee-e886-4f99-eb73-79d5bdb28bcf"
   },
   "outputs": [],
   "source": [
    "def plot_elbow_method(matrix, max_clusters=80):\n",
    "    inertia = []\n",
    "    K_range = range(1, max_clusters + 1)\n",
    "\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42, n_init=10)\n",
    "        kmeans.fit(matrix)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "\n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(K_range, inertia, 'bo-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Calculate the rate of decrease\n",
    "    decrease_rate = np.array([inertia[i-1] - inertia[i] for i in range(1, len(inertia))])\n",
    "    normalized_decrease = decrease_rate / inertia[0]\n",
    "\n",
    "    # Find the elbow point (where the rate of decrease slows down significantly)\n",
    "    elbow_point = np.argmax(np.diff(normalized_decrease)) + 1\n",
    "\n",
    "    plt.axvline(x=elbow_point + 1, color='r', linestyle='--', label=f'Elbow Point: k={elbow_point + 1}')\n",
    "    plt.legend()\n",
    "\n",
    "    return elbow_point + 1\n",
    "\n",
    "# Apply elbow method\n",
    "optimal_k_elbow = plot_elbow_method(matrix_clean)\n",
    "print(f\"Optimal number of clusters according to elbow method: {optimal_k_elbow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKkyxIJUlRPC"
   },
   "source": [
    "### 3.2 Silhouette Analysis\n",
    "\n",
    "The silhouette value measures how similar an object is to its own cluster compared to other clusters. The silhouette ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7waA9XBlRPC",
    "outputId": "ae41ada8-b0d4-438d-add5-aca9e2f48edd"
   },
   "outputs": [],
   "source": [
    "def plot_silhouette_analysis(matrix, max_clusters=80):\n",
    "    K_range = range(2, max_clusters + 1)  # Silhouette requires at least 2 clusters\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(matrix)\n",
    "        silhouette_avg = silhouette_score(matrix, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(K_range, silhouette_scores, 'bo-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Analysis for Optimal k')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Find the optimal number of clusters\n",
    "    optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "    plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k: {optimal_k}')\n",
    "    plt.legend()\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "# Apply silhouette analysis\n",
    "optimal_k_silhouette = plot_silhouette_analysis(matrix_clean)\n",
    "print(f\"Optimal number of clusters according to silhouette analysis: {optimal_k_silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hygrw2etlRPC"
   },
   "source": [
    "### 3.3 Calinski-Harabasz Index\n",
    "\n",
    "The Calinski-Harabasz Index (also known as the Variance Ratio Criterion) is the ratio of the sum of between-cluster dispersion and within-cluster dispersion. Higher values indicate better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPyxJV5glRPC",
    "outputId": "e490b3ea-7614-43c4-d5c7-3c68e32e126e"
   },
   "outputs": [],
   "source": [
    "def plot_calinski_harabasz(matrix, max_clusters=80):\n",
    "    K_range = range(2, max_clusters + 1)  # CH index requires at least 2 clusters\n",
    "    ch_scores = []\n",
    "\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(matrix)\n",
    "        ch_score = calinski_harabasz_score(matrix, labels)\n",
    "        ch_scores.append(ch_score)\n",
    "\n",
    "    # Plot CH scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(K_range, ch_scores, 'bo-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Calinski-Harabasz Index')\n",
    "    plt.title('Calinski-Harabasz Index for Optimal k')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Find the optimal number of clusters\n",
    "    optimal_k = K_range[np.argmax(ch_scores)]\n",
    "    plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k: {optimal_k}')\n",
    "    plt.legend()\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "# Apply Calinski-Harabasz index\n",
    "optimal_k_ch = plot_calinski_harabasz(matrix_clean)\n",
    "print(f\"Optimal number of clusters according to Calinski-Harabasz index: {optimal_k_ch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJhpMVC3lRPD"
   },
   "source": [
    "### 3.4 Davies-Bouldin Index\n",
    "\n",
    "The Davies-Bouldin Index is the average similarity measure of each cluster with its most similar cluster. Lower values indicate better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCKjbnmdlRPD",
    "outputId": "7f3772e2-a00d-4bc8-a8b7-fd9e14c6c1a5"
   },
   "outputs": [],
   "source": [
    "def plot_davies_bouldin(matrix, max_clusters=80):\n",
    "    K_range = range(2, max_clusters + 1)  # DB index requires at least 2 clusters\n",
    "    db_scores = []\n",
    "\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(matrix)\n",
    "        db_score = davies_bouldin_score(matrix, labels)\n",
    "        db_scores.append(db_score)\n",
    "\n",
    "    # Plot DB scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(K_range, db_scores, 'bo-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Davies-Bouldin Index')\n",
    "    plt.title('Davies-Bouldin Index for Optimal k')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Find the optimal number of clusters (minimum DB score)\n",
    "    optimal_k = K_range[np.argmin(db_scores)]\n",
    "    plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k: {optimal_k}')\n",
    "    plt.legend()\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "# Apply Davies-Bouldin index\n",
    "optimal_k_db = plot_davies_bouldin(matrix_clean)\n",
    "print(f\"Optimal number of clusters according to Davies-Bouldin index: {optimal_k_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L8st60KlRPD"
   },
   "source": [
    "### 3.5 Compare Results from Different Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZmKHMGWlRPD",
    "outputId": "78503506-92ae-46d1-ca4c-19fbb5bbd455"
   },
   "outputs": [],
   "source": [
    "# Summarize results\n",
    "methods = ['Elbow Method', 'Silhouette Analysis', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']\n",
    "optimal_k_values = [optimal_k_elbow,\n",
    "                    optimal_k_silhouette, optimal_k_ch, optimal_k_db]\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Method': methods,\n",
    "    'Optimal Number of Clusters': optimal_k_values\n",
    "})\n",
    "\n",
    "print(\"Summary of optimal cluster numbers:\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53dNkdUUlRPD",
    "outputId": "df94b651-ae07-4c8f-88de-e321f6d602cc"
   },
   "outputs": [],
   "source": [
    "# Determine the final optimal number of clusters (e.g., by taking the mode or median)\n",
    "from scipy import stats\n",
    "\n",
    "final_optimal_k = int(stats.mode(optimal_k_values)[0])  # Mode\n",
    "print(f\"Final optimal number of clusters (mode): {final_optimal_k}\")\n",
    "\n",
    "median_optimal_k = int(np.median(optimal_k_values))  # Median\n",
    "print(f\"Final optimal number of clusters (median): {median_optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fChttYZYlRPD"
   },
   "source": [
    "## 4. Final Clustering with Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBxPL1gUlRPE",
    "outputId": "c48b886e-ef0f-4ef0-c6bb-34ddc1dd8749"
   },
   "outputs": [],
   "source": [
    "# Apply K-means with the optimal number of clusters\n",
    "optimal_k = median_optimal_k  # Use the mode as the final optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, init=\"k-means++\", random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(matrix_clean)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df_clean[\"Cluster\"] = labels\n",
    "\n",
    "# Display cluster sizes\n",
    "cluster_sizes = df_clean.groupby(\"Cluster\").size().sort_values(ascending=False)\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rna_aet0lRPE",
    "outputId": "0da7ee7f-1743-4199-f698-d667036efab6"
   },
   "outputs": [],
   "source": [
    "# Visualize the final clusters using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, init=\"random\", learning_rate=200)\n",
    "vis_dims = tsne.fit_transform(matrix_clean)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Use a colormap with distinct colors\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, optimal_k))\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_points = vis_dims[labels == cluster_id]\n",
    "    plt.scatter(\n",
    "        cluster_points[:, 0],\n",
    "        cluster_points[:, 1],\n",
    "        color=colors[cluster_id],\n",
    "        alpha=0.7,\n",
    "        # label=f'Cluster {cluster_id}'\n",
    "    )\n",
    "\n",
    "    # Add cluster centroid\n",
    "    centroid = np.mean(cluster_points, axis=0)\n",
    "    plt.scatter(\n",
    "        centroid[0],\n",
    "        centroid[1],\n",
    "        marker='X',\n",
    "        s=200,\n",
    "        color=colors[cluster_id],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "plt.title(f\"t-SNE Visualization of {optimal_k} Clusters\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZC96fpNlRPE"
   },
   "source": [
    "### 4.1 Analyze Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vltE96rtlRPE",
    "outputId": "7c4efe12-d2e8-46a9-92ab-496b36850e3e"
   },
   "outputs": [],
   "source": [
    "# Display sample items from each cluster\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_samples = df_clean[df_clean['Cluster'] == cluster_id].head(5)\n",
    "    print(f\"\\nCluster {cluster_id} samples:\")\n",
    "    if 'text' in cluster_samples.columns:\n",
    "        for _, row in cluster_samples.iterrows():\n",
    "            print(f\"ID: {row['_id']}\")\n",
    "            print(f\"Text: {row['text'][:200]}...\" if len(str(row['text'])) > 200 else f\"Text: {row['text']}\")\n",
    "            print(\"---\")\n",
    "    else:\n",
    "        display(cluster_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_tpciQIlRPE"
   },
   "source": [
    "### 4.2 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PLhWot6lRPF",
    "outputId": "b8acd059-d7b5-4529-ee22-91c44cdeeb51"
   },
   "outputs": [],
   "source": [
    "# Save the cleaned and clustered data\n",
    "output_columns = ['_id', 'Cluster']\n",
    "if 'text' in df_clean.columns:\n",
    "    output_columns.insert(2, 'text')\n",
    "\n",
    "output_filename = f'data_cleaned_clustered_k{optimal_k}.csv'\n",
    "df_clean.to_csv(output_filename, columns=output_columns, index=False)\n",
    "print(f\"Saved cleaned and clustered data to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters_openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}